{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, you will build a neural network of your own design to evaluate the MNIST dataset.\n",
    "\n",
    "Some of the benchmark results on MNIST include can be found [on Yann LeCun's page](http://yann.lecun.com/exdb/mnist/) and include:\n",
    "\n",
    "88% [Lecun et al., 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "95.3% [Lecun et al., 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "99.65% [Ciresan et al., 2011](http://people.idsia.ch/~juergen/ijcai2011.pdf)\n",
    "\n",
    "MNIST is a great dataset for sanity checking your models, since the accuracy levels achieved by large convolutional neural networks and small linear models are both quite high. This makes it important to be familiar with the data.\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the PATH to include the user installation directory. \n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:/root/.local/bin\"\n",
    "\n",
    "# Restart the Kernel before you move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important: Restart the Kernel before you move on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python-headless==4.5.3.56 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (4.5.3.56)\n",
      "Requirement already satisfied: matplotlib==3.4.3 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: numpy==1.21.2 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.21.2)\n",
      "Requirement already satisfied: pillow==7.0.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (7.0.0)\n",
      "Requirement already satisfied: bokeh==2.1.1 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (2.1.1)\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.11.0)\n",
      "Requirement already satisfied: torchvision==0.12.0 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.12.0)\n",
      "Requirement already satisfied: tqdm==4.63.0 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (4.63.0)\n",
      "Requirement already satisfied: ipywidgets==7.7.0 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (7.7.0)\n",
      "Requirement already satisfied: livelossplot==0.5.4 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (0.5.4)\n",
      "Requirement already satisfied: pytest==7.1.1 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (7.1.1)\n",
      "Requirement already satisfied: pandas==1.3.5 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (1.3.5)\n",
      "Requirement already satisfied: seaborn==0.11.2 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (0.11.2)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (1.0.0)\n",
      "Requirement already satisfied: ipykernel==4.10.0 in /root/.local/lib/python3.7/site-packages (from -r requirements.txt (line 15)) (4.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 2)) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /opt/conda/lib/python3.7/site-packages (from bokeh==2.1.1->-r requirements.txt (line 5)) (2.11.1)\n",
      "Requirement already satisfied: tornado>=5.1 in /opt/conda/lib/python3.7/site-packages (from bokeh==2.1.1->-r requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from bokeh==2.1.1->-r requirements.txt (line 5)) (3.7.4.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.7/site-packages (from bokeh==2.1.1->-r requirements.txt (line 5)) (5.3)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/conda/lib/python3.7/site-packages (from bokeh==2.1.1->-r requirements.txt (line 5)) (20.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.12.0->-r requirements.txt (line 7)) (2.23.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /root/.local/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (3.6.6)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /root/.local/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (3.0.9)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (7.13.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (4.3.3)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets==7.7.0->-r requirements.txt (line 9)) (5.0.4)\n",
      "Requirement already satisfied: iniconfig in /root/.local/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /root/.local/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /root/.local/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (19.3.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /root/.local/lib/python3.7/site-packages (from pytest==7.1.1->-r requirements.txt (line 11)) (1.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.5->-r requirements.txt (line 12)) (2019.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.7/site-packages (from seaborn==0.11.2->-r requirements.txt (line 13)) (1.7.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from jupyter==1.0.0->-r requirements.txt (line 14)) (5.6.1)\n",
      "Requirement already satisfied: qtconsole in /root/.local/lib/python3.7/site-packages (from jupyter==1.0.0->-r requirements.txt (line 14)) (5.4.4)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.7/site-packages (from jupyter==1.0.0->-r requirements.txt (line 14)) (5.7.4)\n",
      "Requirement already satisfied: jupyter-console in /root/.local/lib/python3.7/site-packages (from jupyter==1.0.0->-r requirements.txt (line 14)) (6.6.3)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel==4.10.0->-r requirements.txt (line 15)) (6.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib==3.4.3->-r requirements.txt (line 2)) (45.2.0.post20200209)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib==3.4.3->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.7->bokeh==2.1.1->-r requirements.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 7)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 7)) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 7)) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 7)) (1.25.7)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.1.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (2.5.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (3.0.3)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 9)) (4.6.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 9)) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==7.1.1->-r requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (0.6.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (0.8.4)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (0.4.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (0.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (3.1.1)\n",
      "Requirement already satisfied: pyzmq>=17.1 in /opt/conda/lib/python3.7/site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 14)) (19.0.0)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /root/.local/lib/python3.7/site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 14)) (2.4.1)\n",
      "Requirement already satisfied: Send2Trash in /opt/conda/lib/python3.7/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 14)) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 14)) (0.8.3)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 14)) (0.7.1)\n",
      "Requirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.6.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.1.8)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.7.0->-r requirements.txt (line 9)) (0.15.7)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 14)) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the essential imports you will need \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Specify your transforms as a list if you intend to .\n",
    "The transforms module is already loaded as `transforms`.\n",
    "\n",
    "MNIST is fortunately included in the torchvision module.\n",
    "Then, you can create your dataset using the `MNIST` object from `torchvision.datasets` ([the documentation is available here](https://pytorch.org/vision/stable/datasets.html#mnist)).\n",
    "Make sure to specify `download=True`! \n",
    "\n",
    "Once your dataset is created, you'll also need to define a `DataLoader` from the `torch.utils.data` module for both the train and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally download the data sets, shuffle them and transform each of them. We download the data sets and load them to DataLoader, which combines the data-set and a sampler and provides single- or multi-process iterators over the data-set. A batch size is the number of images we want to read in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define what are the transformations we want to perform on our data before feeding it into the pipeline. In other words, you can consider it to be some kind of custom edit to are performing to the images so that all the images have the same dimensions and properties. We do it using torchvision.transforms.\n",
    "\n",
    "1) transforms.ToTensor() — converts the image into numbers, that are understandable by the system. It separates the image into three color channels (separate images): red, green & blue. Then it converts the pixels of each image to the brightness of their color between 0 and 255. These values are then scaled down to a range between 0 and 1. The image is now a Torch Tensor.\n",
    "\n",
    "2) transforms.Normalize() — normalizes the tensor with a mean and standard deviation which goes as the two parameters respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "Using matplotlib, numpy, and torch, explore the dimensions of your data.\n",
    "\n",
    "You can view images using the `show5` function defined below – it takes a data loader as an argument.\n",
    "Remember that normalized images will look really weird to you! You may want to try changing your transforms to view images.\n",
    "Typically using no transforms other than `toTensor()` works well for viewing – but not as well for training your network.\n",
    "If `show5` doesn't work, go back and check your code for creating your data loaders and your training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains a function for showing 5 images from a dataloader \n",
    "def show5(img_loader):\n",
    "    dataiter = iter(img_loader)\n",
    "    \n",
    "    batch = next(dataiter)\n",
    "    labels = batch[1][0:5]\n",
    "    images = batch[0][0:5]\n",
    "    for i in range(5):\n",
    "        print(int(labels[i].detach()))\n",
    "    \n",
    "        image = images[i].numpy()\n",
    "        plt.imshow(image.T.squeeze().T)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Explore data\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of images as you’ll find out is, torch.Size([64,1,28,28]), which suggests that there are 64 images in each batch and each image has a dimension of 28 x 28 pixels. Similarly, the labels have a shape as torch.Size([64]). 64 images should have 64 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "Using the layers in `torch.nn` (which has been imported as `nn`) and the `torch.nn.functional` module (imported as `F`), construct a neural network based on the parameters of the dataset.\n",
    "Use any architecture you like. \n",
    "\n",
    "*Note*: If you did not flatten your tensors in your transforms or as part of your preprocessing and you are using only `Linear` layers, make sure to use the `Flatten` layer in your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Layer details for the neural network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nn.Sequential wraps the layers in the network. There are three linear layers with ReLU activation. The output layer is a linear layer with LogSoftmax activation because this is a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a loss function and an optimizer, and instantiate the model.\n",
    "\n",
    "If you use a less common loss function, please note why you chose that loss function in a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (5): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your Neural Network\n",
    "Use whatever method you like to train your neural network, and ensure you record the average loss at each epoch. \n",
    "Don't forget to use `torch.device()` and the `.to()` method for both your model and your data if you are using GPU!\n",
    "\n",
    "If you want to print your loss **during** each epoch, you can use the `enumerate` function and print the loss after a set number of batches. 250 batches works well for most people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (5): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss (and validation loss/accuracy, if recorded)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the negative log-likelihood loss. It is useful to train a classification problem with C classes. Together the LogSoftmax() and NLLLoss() acts as the cross-entropy loss as shown in the network architecture diagram above.\n",
    "\n",
    "Also, we have 784 units in the first layer. It is because we flatten out each image before sending it inside the neural network. (28 x 28 = 784). A neural network learns by iterating multiple times over the available data. The terms learn refers to the adjustment of weights of the network to minimize the loss. Let’s visualize how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-1.9611e-03, -1.9611e-03, -1.9611e-03,  ..., -1.9611e-03,\n",
      "         -1.9611e-03, -1.9611e-03],\n",
      "        [ 5.9064e-04,  5.9064e-04,  5.9064e-04,  ...,  5.9064e-04,\n",
      "          5.9064e-04,  5.9064e-04],\n",
      "        [ 1.6424e-03,  1.6424e-03,  1.6424e-03,  ...,  1.6424e-03,\n",
      "          1.6424e-03,  1.6424e-03],\n",
      "        ...,\n",
      "        [-8.9494e-05, -8.9494e-05, -8.9494e-05,  ..., -8.9494e-05,\n",
      "         -8.9494e-05, -8.9494e-05],\n",
      "        [-1.0552e-03, -1.0552e-03, -1.0552e-03,  ..., -1.0552e-03,\n",
      "         -1.0552e-03, -1.0552e-03],\n",
      "        [ 5.1266e-04,  5.1266e-04,  5.1266e-04,  ...,  5.1266e-04,\n",
      "          5.1266e-04,  5.1266e-04]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images.cuda())\n",
    "loss = criterion(logps, labels.cuda())\n",
    "\n",
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the backward pass, the model weights are set to default none values. Once, we call the backward() function the weights are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving your model\n",
    "\n",
    "Once your model is done training, try tweaking your hyperparameters and training again below to improve your accuracy on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0330, -0.0060,  0.0323,  ..., -0.0151,  0.0245, -0.0260],\n",
      "        [ 0.0180, -0.0287,  0.0340,  ..., -0.0085, -0.0211, -0.0303],\n",
      "        [ 0.0223,  0.0153, -0.0009,  ...,  0.0103, -0.0354, -0.0162],\n",
      "        ...,\n",
      "        [-0.0268, -0.0035,  0.0241,  ...,  0.0143,  0.0134, -0.0345],\n",
      "        [ 0.0277, -0.0039, -0.0158,  ...,  0.0176,  0.0027, -0.0050],\n",
      "        [-0.0209,  0.0168, -0.0070,  ..., -0.0355, -0.0168, -0.0129]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Gradient - tensor([[-0.0014, -0.0014, -0.0014,  ..., -0.0014, -0.0014, -0.0014],\n",
      "        [-0.0026, -0.0026, -0.0026,  ..., -0.0026, -0.0026, -0.0026],\n",
      "        [ 0.0006,  0.0006,  0.0006,  ...,  0.0006,  0.0006,  0.0006],\n",
      "        ...,\n",
      "        [ 0.0004,  0.0004,  0.0004,  ...,  0.0004,  0.0004,  0.0004],\n",
      "        [ 0.0029,  0.0029,  0.0029,  ...,  0.0029,  0.0029,  0.0029],\n",
      "        [ 0.0006,  0.0006,  0.0006,  ...,  0.0006,  0.0006,  0.0006]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model(images.cuda())\n",
    "loss = criterion(output, labels.cuda())\n",
    "loss.backward()\n",
    "print('Gradient -', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your neural network iterates over the training set and updates the weights. We make use of torch.optim which is a module provided by PyTorch to optimize the model, perform gradient descent and update the weights by back-propagation. Thus in each epoch (number of times we iterate over the training set), we will be seeing a gradual decrease in training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.6579585287775567\n",
      "Epoch 1 - Training loss: 0.28457362963351357\n",
      "Epoch 2 - Training loss: 0.22247558944562731\n",
      "Epoch 3 - Training loss: 0.18058902815357644\n",
      "Epoch 4 - Training loss: 0.15048015387113223\n",
      "Epoch 5 - Training loss: 0.13025088784600625\n",
      "Epoch 6 - Training loss: 0.11397976528849206\n",
      "Epoch 7 - Training loss: 0.09895255197999256\n",
      "Epoch 8 - Training loss: 0.09075742832688825\n",
      "Epoch 9 - Training loss: 0.08146335607243659\n",
      "Epoch 10 - Training loss: 0.07311463374783521\n",
      "Epoch 11 - Training loss: 0.0671963700443816\n",
      "Epoch 12 - Training loss: 0.06115572714358012\n",
      "Epoch 13 - Training loss: 0.057037625799644616\n",
      "Epoch 14 - Training loss: 0.05286507327943595\n",
      "\n",
      "Training Time (in minutes) = 3.0663389841715496\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images.cuda())\n",
    "        loss = criterion(output, labels.cuda())\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing your model\n",
    "Using the previously created DataLoader for the test set, compute the percentage of correct predictions using the highest probability prediction.\n",
    "\n",
    "If your accuracy is over 90%, great work, but see if you can push a bit further! If your accuracy is under 90%, you'll need to make improvements. Go back and check your model architecture, loss function, and optimizer to make sure they're appropriate for an image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The model is ready, but we have to evaluate it first. I created a utility function view_classify() to show the image and class probabilities that were predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.cpu().data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I passed an image to the trained model from the validation set that we created earlier, to see how the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit = 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(valloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img.cuda())\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.cpu().numpy()[0])\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate through the validation set using a for loop and calculate the total number of correct predictions. This is how we can calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9764\n"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for images,labels in valloader:\n",
    "  for i in range(len(labels)):\n",
    "    img = images[i].view(1, 784)\n",
    "    # Turn off gradients to speed up this part\n",
    "    with torch.no_grad():\n",
    "        logps = model(img.cuda())\n",
    "\n",
    "    # Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.cpu().numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got over 97.64 % accuracy! The reason we got such a high accuracy was that our data-set was clean, had a variety of well-shuffled images and a large number of them. This made our model well prepared to recognize a large number of unseen digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model, './my_mnist_model.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter is the model object, the second parameter is the path. PyTorch models are generally saved with .pt or .pth extension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
